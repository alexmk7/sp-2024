# Kafka Producers/Consumers

С помощью какой-либо клиентской библиотеки (например [confluent-kafka](https://pypi.org/project/confluent-kafka/), [kafka-python](https://pypi.org/project/kafka-python/), [aiokafka](https://pypi.org/project/aiokafka/) или [faust](https://pypi.org/project/faust-streaming/) симулируйте процесс работы датчиков IoT устройств.

В отдельном скрипте работают "производители":
- датчики (их число настраивается) с определенными задержками пишут в один топик показания, каждый согласно какому-то настаиваемому распределению с настраиваемыми задержками по времени. Сообщения пишутся в произвольном формате, должны быть следующие поля: время события, "тип" датчика (температура, давление, и т.п.), имя датчика, значение (число с плавающей точкой). 

В другом скрипте работают "потребители":
- данные накапливаются и каждые 20 секунд (настраиваемый параметр) строятся два pandas DataFrame - среднее значение показаний по типу датчика и по имени датчика. Данные выводятся в консоль.

# Spark Streaming

Выберете несколько популярных Telegram канала, с помощью какой-нибудь клиентской библиотеки (например [Telethone](https://github.com/LonamiWebs/Telethon)) научитесь получать свежие сообщения и записывать из в файлы (parquet). Должны быть следующие поля: время, источник (канал), текст сообщения, есть ли медиа. Напишите соответствующий скрипт.

Файл со свежими сообщениями появляется каждую минуту (настраиваемый параметр), в имени содержится временной промежуток накопления сообщений. 

В другом скрипте, с помощью Spark Structured Streaming научитесь находить аномалии (обсуждение темы, всплеск частоты сообщений и т.п.) в собираемых данных. 
